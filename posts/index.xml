<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Hi! I&#39;m Mohammed Tayeh ðŸ˜„</title>
        <link>https://tayeh.me/posts/</link>
        <description>Recent content in Posts on Hi! I&#39;m Mohammed Tayeh ðŸ˜„</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
        <lastBuildDate>Sun, 06 Feb 2022 17:31:50 +0200</lastBuildDate>
        <atom:link href="https://tayeh.me/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Install Kubernetes Cluster on Centos 8 With Kubeadm and CRI-O</title>
            <link>https://tayeh.me/posts/install-kubernetes-cluster-on-centos-8-with-kubeadm-crio/</link>
            <pubDate>Sun, 06 Feb 2022 17:31:50 +0200</pubDate>
            
            <guid>https://tayeh.me/posts/install-kubernetes-cluster-on-centos-8-with-kubeadm-crio/</guid>
            <description>This guide will teach you how to deploy a Kubernetes Cluster on CentOS 8 by using kubeadm and with CRI-O Container runtime
Will use one VM as a master node and 2 VM as a worker nodes
   VM Role IP Hostname Resource     Master 192.168.151.128 master01.tayeh.me 2GB Ram, 2vcpus   Worker 01 192.168.151.129 worker01.tayeh.me 2GB Ram, 2vcpus   Worker 02 192.168.151.130 worker02.tayeh.me 2GB Ram, 2vcpus    Steps 1.</description>
            <content type="html"><![CDATA[<p>This guide will teach you how to deploy a Kubernetes Cluster on CentOS 8 by using kubeadm and with CRI-O Container runtime</p>
<p>Will use one VM as a master node and 2 VM as a worker nodes</p>
<table>
<thead>
<tr>
<th>VM Role</th>
<th>IP</th>
<th>Hostname</th>
<th>Resource</th>
</tr>
</thead>
<tbody>
<tr>
<td>Master</td>
<td>192.168.151.128</td>
<td>master01.tayeh.me</td>
<td>2GB Ram, 2vcpus</td>
</tr>
<tr>
<td>Worker 01</td>
<td>192.168.151.129</td>
<td>worker01.tayeh.me</td>
<td>2GB Ram, 2vcpus</td>
</tr>
<tr>
<td>Worker 02</td>
<td>192.168.151.130</td>
<td>worker02.tayeh.me</td>
<td>2GB Ram, 2vcpus</td>
</tr>
</tbody>
</table>
<h3 id="steps">Steps</h3>
<h4 id="1-set-hostname-on-vms">1. Set hostname on VM&rsquo;s</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ hostnamectl set-hostname master01.tayeh.me
</code></pre></div><h4 id="2-disable-swap">2. Disable Swap</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ swapoff -a
$ vi /etc/fstab <span style="color:#75715e"># and remove the line containe swap </span>
</code></pre></div><h4 id="3-disable-selinux">3. Disable SELinux</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ setenforce <span style="color:#ae81ff">0</span>
$ vi /etc/selinux/config <span style="color:#75715e"># and set SELINUX=disabled </span>
</code></pre></div><p>&ldquo;reboot VM&rsquo;s after disable the SELinux&rdquo;</p>
<h4 id="4-configure-sysctl-and-enable-kernel-mod">4. Configure sysctl and enable kernel mod</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ modprobe overlay
$ modprobe br_netfilter
tee /etc/sysctl.d/k8s.conf<span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-iptables = 1
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-ip6tables = 1
</span><span style="color:#e6db74">net.ipv4.ip_forward = 1
</span><span style="color:#e6db74">EOF</span>
$ sysctl --system
</code></pre></div><h4 id="5-install-kubelet-kubeadm-and-kubectl-and-enable-epel-release">5. Install kubelet, kubeadm and kubectl (and enable epel-release)</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ tee /etc/yum.repos.d/kubernetes.repo<span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74">[kubernetes]
</span><span style="color:#e6db74">name=Kubernetes
</span><span style="color:#e6db74">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
</span><span style="color:#e6db74">enabled=1
</span><span style="color:#e6db74">gpgcheck=1
</span><span style="color:#e6db74">repo_gpgcheck=1
</span><span style="color:#e6db74">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ dnf -y install kubelet kubeadm kubectl --disableexcludes<span style="color:#f92672">=</span>kubernetes epel-release
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl version --client <span style="color:#75715e"># check the version of kubectl</span>
Client Version: version.Info<span style="color:#f92672">{</span>Major:<span style="color:#e6db74">&#34;1&#34;</span>, Minor:<span style="color:#e6db74">&#34;23&#34;</span>, GitVersion:<span style="color:#e6db74">&#34;v1.23.3&#34;</span>, GitCommit:<span style="color:#e6db74">&#34;816c97ab8cff8a1c72eccca1026f7820e93e0d25&#34;</span>, GitTreeState:<span style="color:#e6db74">&#34;clean&#34;</span>, BuildDate:<span style="color:#e6db74">&#34;2022-01-25T21:25:17Z&#34;</span>, GoVersion:<span style="color:#e6db74">&#34;go1.17.6&#34;</span>, Compiler:<span style="color:#e6db74">&#34;gc&#34;</span>, Platform:<span style="color:#e6db74">&#34;linux/amd64&#34;</span><span style="color:#f92672">}</span>
</code></pre></div><h4 id="6-install-container-runtime-cri-o">6. Install Container runtime (CRI-O)</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ export OS<span style="color:#f92672">=</span>CentOS_8_Stream <span style="color:#75715e"># or OS=CentOS_8</span>
$ export VERSION<span style="color:#f92672">=</span>1.23 <span style="color:#75715e"># it&#39;s must matching your kubernetes version</span>
<span style="color:#75715e"># Add repo</span>
$ curl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable.repo https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/devel:kubic:libcontainers:stable.repo
$ curl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$VERSION/$OS/devel:kubic:libcontainers:stable:cri-o:$VERSION.repo

<span style="color:#75715e"># Install CRI-O</span>
$ dnf install cri-o

<span style="color:#75715e"># Start and enable Service</span>
$ systemctl daemon-reload
$ systemctl enable --now crio
$ systemctl status crio
</code></pre></div><h4 id="7-firewalld-rule">7. Firewalld Rule</h4>
<p>on master server</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ firewall-cmd --add-port<span style="color:#f92672">={</span>6443,2379-2380,10250,10251,10252,5473,179,5473<span style="color:#f92672">}</span>/tcp --permanent
$ firewall-cmd --add-port<span style="color:#f92672">={</span>4789,8285,8472<span style="color:#f92672">}</span>/udp --permanent
$ firewall-cmd --reload
</code></pre></div><p>on worker server</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ firewall-cmd --add-port<span style="color:#f92672">={</span>10250,30000-32767,5473,179,5473<span style="color:#f92672">}</span>/tcp --permanent
$ firewall-cmd --add-port<span style="color:#f92672">={</span>4789,8285,8472<span style="color:#f92672">}</span>/udp --permanent
$ firewall-cmd --reload
</code></pre></div><h4 id="8-install-control-plane-node">8. install control-plane node</h4>
<p>make sure the br_netfilter module is loaded</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ lsmod | grep br_netfilter
br_netfilter           <span style="color:#ae81ff">24576</span>  <span style="color:#ae81ff">0</span>
bridge                <span style="color:#ae81ff">278528</span>  <span style="color:#ae81ff">1</span> br_netfilter
<span style="color:#75715e"># if you not see the output like above rerun this commands ($ modprobe overlay; modprobe br_netfilter)</span>
</code></pre></div><p><strong>Enable kubelet service</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ systemctl enable kubelet
</code></pre></div><p><strong>Pull container images</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubeadm config images pull
<span style="color:#f92672">[</span>config/images<span style="color:#f92672">]</span> Pulled k8s.gcr.io/kube-apiserver:v1.23.3
<span style="color:#f92672">[</span>config/images<span style="color:#f92672">]</span> Pulled k8s.gcr.io/kube-controller-manager:v1.23.3
<span style="color:#f92672">[</span>config/images<span style="color:#f92672">]</span> Pulled k8s.gcr.io/kube-scheduler:v1.23.3
<span style="color:#f92672">[</span>config/images<span style="color:#f92672">]</span> Pulled k8s.gcr.io/kube-proxy:v1.23.3
<span style="color:#f92672">[</span>config/images<span style="color:#f92672">]</span> Pulled k8s.gcr.io/pause:3.6
<span style="color:#f92672">[</span>config/images<span style="color:#f92672">]</span> Pulled k8s.gcr.io/etcd:3.5.1-0
<span style="color:#f92672">[</span>config/images<span style="color:#f92672">]</span> Pulled k8s.gcr.io/coredns/coredns:v1.8.6
</code></pre></div><p><strong>Create cluster</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubeadm init
W0206 20:14:42.692845    <span style="color:#ae81ff">2943</span> version.go:103<span style="color:#f92672">]</span> could not fetch a Kubernetes version from the internet: unable to get URL <span style="color:#e6db74">&#34;https://dl.k8s.io/release/stable-1.txt&#34;</span>: Get <span style="color:#e6db74">&#34;https://dl.k8s.io/release/stable-1.txt&#34;</span>: context deadline exceeded <span style="color:#f92672">(</span>Client.Timeout exceeded <span style="color:#66d9ef">while</span> awaiting headers<span style="color:#f92672">)</span>
W0206 20:14:42.692944    <span style="color:#ae81ff">2943</span> version.go:104<span style="color:#f92672">]</span> falling back to the local client version: v1.23.3
<span style="color:#f92672">[</span>init<span style="color:#f92672">]</span> Using Kubernetes version: v1.23.3
<span style="color:#f92672">[</span>preflight<span style="color:#f92672">]</span> Running pre-flight checks
        <span style="color:#f92672">[</span>WARNING Firewalld<span style="color:#f92672">]</span>: firewalld is active, please ensure ports <span style="color:#f92672">[</span><span style="color:#ae81ff">6443</span> 10250<span style="color:#f92672">]</span> are open or your cluster may not <span style="color:#66d9ef">function</span> correctly
        <span style="color:#f92672">[</span>WARNING FileExisting-tc<span style="color:#f92672">]</span>: tc not found in system path
        <span style="color:#f92672">[</span>WARNING Hostname<span style="color:#f92672">]</span>: hostname <span style="color:#e6db74">&#34;master01.tayeh.me&#34;</span> could not be reached
        <span style="color:#f92672">[</span>WARNING Hostname<span style="color:#f92672">]</span>: hostname <span style="color:#e6db74">&#34;master01.tayeh.me&#34;</span>: lookup master01.tayeh.me on 192.168.151.2:53: read udp 192.168.151.128:54368-&gt;192.168.151.2:53: i/o timeout
<span style="color:#f92672">[</span>preflight<span style="color:#f92672">]</span> Pulling images required <span style="color:#66d9ef">for</span> setting up a Kubernetes cluster
<span style="color:#f92672">[</span>preflight<span style="color:#f92672">]</span> This might take a minute or two, depending on the speed of your internet connection
<span style="color:#f92672">[</span>preflight<span style="color:#f92672">]</span> You can also perform this action in beforehand using <span style="color:#e6db74">&#39;kubeadm config images pull&#39;</span>
<span style="color:#f92672">[</span>certs<span style="color:#f92672">]</span> Using certificateDir folder <span style="color:#e6db74">&#34;/etc/kubernetes/pki&#34;</span>
<span style="color:#f92672">[</span>certs<span style="color:#f92672">]</span> Generating <span style="color:#e6db74">&#34;ca&#34;</span> certificate and key
<span style="color:#f92672">[</span>certs<span style="color:#f92672">]</span> Generating <span style="color:#e6db74">&#34;apiserver&#34;</span> certificate and key
<span style="color:#f92672">[</span>certs<span style="color:#f92672">]</span> apiserver serving cert is signed <span style="color:#66d9ef">for</span> DNS names <span style="color:#f92672">[</span>kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master01.tayeh.me<span style="color:#f92672">]</span> and IPs <span style="color:#f92672">[</span>10.96.0.1 192.168.151.128<span style="color:#f92672">]</span>
<span style="color:#f92672">[</span>certs<span style="color:#f92672">]</span> Generating <span style="color:#e6db74">&#34;apiserver-kubelet-client&#34;</span> certificate and key
<span style="color:#f92672">[</span>certs<span style="color:#f92672">]</span> Generating <span style="color:#e6db74">&#34;front-proxy-ca&#34;</span> certificate and key
<span style="color:#f92672">[</span>certs<span style="color:#f92672">]</span> Generating <span style="color:#e6db74">&#34;front-proxy-client&#34;</span> certificate and key
<span style="color:#f92672">[</span>certs<span style="color:#f92672">]</span> Generating <span style="color:#e6db74">&#34;etcd/ca&#34;</span> certificate and key
<span style="color:#f92672">[</span>certs<span style="color:#f92672">]</span> Generating <span style="color:#e6db74">&#34;etcd/server&#34;</span> certificate and key
<span style="color:#f92672">[</span>certs<span style="color:#f92672">]</span> etcd/server serving cert is signed <span style="color:#66d9ef">for</span> DNS names <span style="color:#f92672">[</span>localhost master01.tayeh.me<span style="color:#f92672">]</span> and IPs <span style="color:#f92672">[</span>192.168.151.128 127.0.0.1 ::1<span style="color:#f92672">]</span>
<span style="color:#f92672">[</span>certs<span style="color:#f92672">]</span> Generating <span style="color:#e6db74">&#34;etcd/peer&#34;</span> certificate and key
<span style="color:#f92672">[</span>certs<span style="color:#f92672">]</span> etcd/peer serving cert is signed <span style="color:#66d9ef">for</span> DNS names <span style="color:#f92672">[</span>localhost master01.tayeh.me<span style="color:#f92672">]</span> and IPs <span style="color:#f92672">[</span>192.168.151.128 127.0.0.1 ::1<span style="color:#f92672">]</span>
<span style="color:#f92672">[</span>certs<span style="color:#f92672">]</span> Generating <span style="color:#e6db74">&#34;etcd/healthcheck-client&#34;</span> certificate and key
<span style="color:#f92672">[</span>certs<span style="color:#f92672">]</span> Generating <span style="color:#e6db74">&#34;apiserver-etcd-client&#34;</span> certificate and key
<span style="color:#f92672">[</span>certs<span style="color:#f92672">]</span> Generating <span style="color:#e6db74">&#34;sa&#34;</span> key and public key
<span style="color:#f92672">[</span>kubeconfig<span style="color:#f92672">]</span> Using kubeconfig folder <span style="color:#e6db74">&#34;/etc/kubernetes&#34;</span>
<span style="color:#f92672">[</span>kubeconfig<span style="color:#f92672">]</span> Writing <span style="color:#e6db74">&#34;admin.conf&#34;</span> kubeconfig file
<span style="color:#f92672">[</span>kubeconfig<span style="color:#f92672">]</span> Writing <span style="color:#e6db74">&#34;kubelet.conf&#34;</span> kubeconfig file
<span style="color:#f92672">[</span>kubeconfig<span style="color:#f92672">]</span> Writing <span style="color:#e6db74">&#34;controller-manager.conf&#34;</span> kubeconfig file
<span style="color:#f92672">[</span>kubeconfig<span style="color:#f92672">]</span> Writing <span style="color:#e6db74">&#34;scheduler.conf&#34;</span> kubeconfig file
<span style="color:#f92672">[</span>kubelet-start<span style="color:#f92672">]</span> Writing kubelet environment file with flags to file <span style="color:#e6db74">&#34;/var/lib/kubelet/kubeadm-flags.env&#34;</span>
<span style="color:#f92672">[</span>kubelet-start<span style="color:#f92672">]</span> Writing kubelet configuration to file <span style="color:#e6db74">&#34;/var/lib/kubelet/config.yaml&#34;</span>
<span style="color:#f92672">[</span>kubelet-start<span style="color:#f92672">]</span> Starting the kubelet
<span style="color:#f92672">[</span>control-plane<span style="color:#f92672">]</span> Using manifest folder <span style="color:#e6db74">&#34;/etc/kubernetes/manifests&#34;</span>
<span style="color:#f92672">[</span>control-plane<span style="color:#f92672">]</span> Creating static Pod manifest <span style="color:#66d9ef">for</span> <span style="color:#e6db74">&#34;kube-apiserver&#34;</span>
<span style="color:#f92672">[</span>control-plane<span style="color:#f92672">]</span> Creating static Pod manifest <span style="color:#66d9ef">for</span> <span style="color:#e6db74">&#34;kube-controller-manager&#34;</span>
<span style="color:#f92672">[</span>control-plane<span style="color:#f92672">]</span> Creating static Pod manifest <span style="color:#66d9ef">for</span> <span style="color:#e6db74">&#34;kube-scheduler&#34;</span>
<span style="color:#f92672">[</span>etcd<span style="color:#f92672">]</span> Creating static Pod manifest <span style="color:#66d9ef">for</span> local etcd in <span style="color:#e6db74">&#34;/etc/kubernetes/manifests&#34;</span>
<span style="color:#f92672">[</span>wait-control-plane<span style="color:#f92672">]</span> Waiting <span style="color:#66d9ef">for</span> the kubelet to boot up the control plane as static Pods from directory <span style="color:#e6db74">&#34;/etc/kubernetes/manifests&#34;</span>. This can take up to 4m0s
<span style="color:#f92672">[</span>apiclient<span style="color:#f92672">]</span> All control plane components are healthy after 11.002001 seconds
<span style="color:#f92672">[</span>upload-config<span style="color:#f92672">]</span> Storing the configuration used in ConfigMap <span style="color:#e6db74">&#34;kubeadm-config&#34;</span> in the <span style="color:#e6db74">&#34;kube-system&#34;</span> Namespace
<span style="color:#f92672">[</span>kubelet<span style="color:#f92672">]</span> Creating a ConfigMap <span style="color:#e6db74">&#34;kubelet-config-1.23&#34;</span> in namespace kube-system with the configuration <span style="color:#66d9ef">for</span> the kubelets in the cluster
NOTE: The <span style="color:#e6db74">&#34;kubelet-config-1.23&#34;</span> naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just <span style="color:#e6db74">&#34;kubelet-config&#34;</span>. Kubeadm upgrade will handle this transition transparently.
<span style="color:#f92672">[</span>upload-certs<span style="color:#f92672">]</span> Skipping phase. Please see --upload-certs
<span style="color:#f92672">[</span>mark-control-plane<span style="color:#f92672">]</span> Marking the node master01.tayeh.me as control-plane by adding the labels: <span style="color:#f92672">[</span>node-role.kubernetes.io/master<span style="color:#f92672">(</span>deprecated<span style="color:#f92672">)</span> node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers<span style="color:#f92672">]</span>
<span style="color:#f92672">[</span>mark-control-plane<span style="color:#f92672">]</span> Marking the node master01.tayeh.me as control-plane by adding the taints <span style="color:#f92672">[</span>node-role.kubernetes.io/master:NoSchedule<span style="color:#f92672">]</span>
<span style="color:#f92672">[</span>bootstrap-token<span style="color:#f92672">]</span> Using token: ddyzgx.nuhs3eyyhm3disy7
<span style="color:#f92672">[</span>bootstrap-token<span style="color:#f92672">]</span> Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
<span style="color:#f92672">[</span>bootstrap-token<span style="color:#f92672">]</span> configured RBAC rules to allow Node Bootstrap tokens to get nodes
<span style="color:#f92672">[</span>bootstrap-token<span style="color:#f92672">]</span> configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order <span style="color:#66d9ef">for</span> nodes to get long term certificate credentials
<span style="color:#f92672">[</span>bootstrap-token<span style="color:#f92672">]</span> configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
<span style="color:#f92672">[</span>bootstrap-token<span style="color:#f92672">]</span> configured RBAC rules to allow certificate rotation <span style="color:#66d9ef">for</span> all node client certificates in the cluster
<span style="color:#f92672">[</span>bootstrap-token<span style="color:#f92672">]</span> Creating the <span style="color:#e6db74">&#34;cluster-info&#34;</span> ConfigMap in the <span style="color:#e6db74">&#34;kube-public&#34;</span> namespace
<span style="color:#f92672">[</span>kubelet-finalize<span style="color:#f92672">]</span> Updating <span style="color:#e6db74">&#34;/etc/kubernetes/kubelet.conf&#34;</span> to point to a rotatable kubelet client certificate and key
<span style="color:#f92672">[</span>addons<span style="color:#f92672">]</span> Applied essential addon: CoreDNS
<span style="color:#f92672">[</span>addons<span style="color:#f92672">]</span> Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown <span style="color:#66d9ef">$(</span>id -u<span style="color:#66d9ef">)</span>:<span style="color:#66d9ef">$(</span>id -g<span style="color:#66d9ef">)</span> $HOME/.kube/config

Alternatively, <span style="color:#66d9ef">if</span> you are the root user, you can run:

  export KUBECONFIG<span style="color:#f92672">=</span>/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run <span style="color:#e6db74">&#34;kubectl apply -f [podnetwork].yaml&#34;</span> with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.151.128:6443 --token ddyzgx.nuhs3eyyhm3disy7 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>        --discovery-token-ca-cert-hash sha256:bd4cf138e1e0b018ebeb5c34074354d5eeee90082468728ee165c6be2bfc1d69
</code></pre></div><p><strong>Configure kubectl using commands in the output</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ mkdir -p $HOME/.kube
$ cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ chown <span style="color:#66d9ef">$(</span>id -u<span style="color:#66d9ef">)</span>:<span style="color:#66d9ef">$(</span>id -g<span style="color:#66d9ef">)</span> $HOME/.kube/config
</code></pre></div><p><strong>Check cluster status</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl cluster-info
Kubernetes control plane is running at https://192.168.151.128:6443
CoreDNS is running at https://192.168.151.128:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

$ kubectl get node 
NAME                STATUS   ROLES                  AGE     VERSION
master01.tayeh.me   Ready    control-plane,master   8m32s   v1.23.3
</code></pre></div><h4 id="9-install-network-plugin">9. Install network plugin</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
</code></pre></div><p><strong>check the pods rununing</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl get pods --all-namespaces
NAMESPACE     NAME                                        READY   STATUS              RESTARTS   AGE
kube-system   calico-kube-controllers-566dc76669-dm8ph    0/1     ContainerCreating   <span style="color:#ae81ff">0</span>          20s
kube-system   calico-node-2qphl                           0/1     Init:0/3            <span style="color:#ae81ff">0</span>          21s
kube-system   coredns-64897985d-b2xsl                     1/1     Running             <span style="color:#ae81ff">0</span>          14m
kube-system   coredns-64897985d-kjb7z                     1/1     Running             <span style="color:#ae81ff">0</span>          14m
kube-system   etcd-master01.tayeh.me                      1/1     Running             <span style="color:#ae81ff">0</span>          14m
kube-system   kube-apiserver-master01.tayeh.me            1/1     Running             <span style="color:#ae81ff">0</span>          14m
kube-system   kube-controller-manager-master01.tayeh.me   1/1     Running             <span style="color:#ae81ff">0</span>          14m
kube-system   kube-proxy-ksq56                            1/1     Running             <span style="color:#ae81ff">0</span>          14m
kube-system   kube-scheduler-master01.tayeh.me            1/1     Running             <span style="color:#ae81ff">0</span>          14m
</code></pre></div><h4 id="10-add-worker-node">10. Add Worker Node</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubeadm join 192.168.151.128:6443 --token ddyzgx.nuhs3eyyhm3disy7 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>        --discovery-token-ca-cert-hash sha256:bd4cf138e1e0b018ebeb5c34074354d5eeee90082468728ee165c6be2bfc1d69 
<span style="color:#75715e"># this command from the output of (kubeadm init)</span>
</code></pre></div><p><strong>check nodes</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl get node  -o wide
NAME                STATUS   ROLES                  AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE          KERNEL-VERSION          CONTAINER-RUNTIME
master01.tayeh.me   Ready    control-plane,master   17m   v1.23.3   192.168.151.128   &lt;none&gt;        CentOS Stream <span style="color:#ae81ff">8</span>   4.18.0-358.el8.x86_64   cri-o://1.23.0
worker01.tayeh.me   Ready    &lt;none&gt;                 28s   v1.23.3   192.168.151.129   &lt;none&gt;        CentOS Stream <span style="color:#ae81ff">8</span>   4.18.0-358.el8.x86_64   cri-o://1.23.0
worker02.tayeh.me   Ready    &lt;none&gt;                 23s   v1.23.3   192.168.151.130   &lt;none&gt;        CentOS Stream <span style="color:#ae81ff">8</span>   4.18.0-358.el8.x86_64   cri-o://1.23.0
</code></pre></div><h4 id="11-try-deploy-application-on-cluster">11. try Deploy application on cluster</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl apply -f https://k8s.io/examples/pods/commands.yaml
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl get pods -o wide
NAME           READY   STATUS              RESTARTS   AGE   IP       NODE                NOMINATED NODE   READINESS GATES
command-demo   0/1     ContainerCreating   <span style="color:#ae81ff">0</span>          53s   &lt;none&gt;   worker01.tayeh.me   &lt;none&gt;           &lt;none&gt;

$ kubectl get pods -o wide 
NAME           READY   STATUS      RESTARTS   AGE     IP          NODE                NOMINATED NODE   READINESS GATES
command-demo   0/1     Completed   <span style="color:#ae81ff">0</span>          7m29s   10.85.0.2   worker01.tayeh.me   &lt;none&gt;           &lt;none&gt;
</code></pre></div><blockquote>
<p>enjoy</p>
</blockquote>
]]></content>
        </item>
        
        <item>
            <title>Digitalocean Kubernetes Challenge 2021 - Harbor</title>
            <link>https://tayeh.me/posts/digitalocean-kubernetes-challenge-2021/</link>
            <pubDate>Tue, 07 Dec 2021 18:52:22 +0200</pubDate>
            
            <guid>https://tayeh.me/posts/digitalocean-kubernetes-challenge-2021/</guid>
            <description>Introduction In this is guide i will explain how to create a Kubernetes Cluster on Digitalocean using Terraform, then i will deploy the Harbor on Cluster as a internal container registry
Terraform Files the terraform code used to create the cluster
resource &amp;#34;digitalocean_kubernetes_cluster&amp;#34; &amp;#34;tayeh-cluster&amp;#34; { name = &amp;#34;tayeh-cluster&amp;#34; region = &amp;#34;fra1&amp;#34; version = &amp;#34;1.21.5-do.0&amp;#34; node_pool { name = &amp;#34;worker-pool&amp;#34; size = &amp;#34;s-1vcpu-2gb&amp;#34; node_count = 3 auto_scale = true min_nodes = 3 max_nodes = 4 } } after prepare the Terraform code execute this command to apply it</description>
            <content type="html"><![CDATA[<h3 id="introduction">Introduction</h3>
<p>In this is guide i will explain how to create a Kubernetes Cluster on Digitalocean using Terraform, then i will deploy the Harbor on Cluster as a internal container registry</p>
<h3 id="terraform-files">Terraform Files</h3>
<p>the terraform code used to create the cluster</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rb" data-lang="rb">resource <span style="color:#e6db74">&#34;digitalocean_kubernetes_cluster&#34;</span> <span style="color:#e6db74">&#34;tayeh-cluster&#34;</span> {
  name   <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;tayeh-cluster&#34;</span>
  region <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;fra1&#34;</span>
  version <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;1.21.5-do.0&#34;</span>

  node_pool {
    name       <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;worker-pool&#34;</span>
    size       <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;s-1vcpu-2gb&#34;</span>
    node_count <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
    auto_scale <span style="color:#f92672">=</span> <span style="color:#66d9ef">true</span>
    min_nodes  <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
    max_nodes  <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>
  }
}
</code></pre></div><p>after prepare the Terraform code execute this command to apply it</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">terraform init
export TF_VAR_do_token<span style="color:#f92672">=</span>&lt;do_token&gt;
terraform apply
</code></pre></div><p>now you can download the config on Digitalocean Kubernetes Cluster page, and try to connect to your cluster</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>tayeh@fedora ~<span style="color:#f92672">]</span>$ kubectl --kubeconfig<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;tayeh-cluster-kubeconfig.yaml&#34;</span> get nodes
NAME                STATUS   ROLES    AGE   VERSION
worker-pool-uwtu0   Ready    &lt;none&gt;   25m   v1.21.5
worker-pool-uwtu1   Ready    &lt;none&gt;   25m   v1.21.5
worker-pool-uwtuz   Ready    &lt;none&gt;   25m   v1.21.5
</code></pre></div><h3 id="deploy-harbor-to-cluster">deploy Harbor to Cluster</h3>
<p>first we need to install Helm using this command</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod <span style="color:#ae81ff">700</span> get_helm.sh 
./get_helm.sh
</code></pre></div><p><em>Install <code>ingress-nginx</code> controller</em></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm upgrade --install ingress-nginx ingress-nginx <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --repo https://kubernetes.github.io/ingress-nginx <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --namespace ingress-nginx --create-namespace <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --kubeconfig<span style="color:#f92672">=</span>tayeh-cluster-kubeconfig.yaml
</code></pre></div><p><em>Get Nginx IP</em></p>
<pre><code>kubectl get svc -n ingress-nginx --kubeconfig=tayeh-cluster-kubeconfig.yaml
</code></pre><blockquote>
<p>point your DNS to the EXTERNAL-IP</p>
</blockquote>
<p><em>harbor</em>
create value file</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">expose</span>:
  <span style="color:#f92672">type</span>: <span style="color:#ae81ff">ingress</span>
  <span style="color:#f92672">tls</span>:
    <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">ingress</span>:
    <span style="color:#f92672">hosts</span>:
      <span style="color:#f92672">core</span>: <span style="color:#ae81ff">harbor.tayeh.me</span>
      <span style="color:#f92672">notary</span>: <span style="color:#ae81ff">notary.tayeh.me</span>
<span style="color:#f92672">externalURL</span>: <span style="color:#ae81ff">http://harbor.tayeh.me</span>
<span style="color:#f92672">harborAdminPassword</span>: <span style="color:#e6db74">&#34;P@ssw0rd&#34;</span>
</code></pre></div><p>then <em>Download Chart</em></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm repo add harbor https://helm.goharbor.io
helm install harbor harbor/harbor --values harbor.yml -n harbor --create-namespace --kubeconfig<span style="color:#f92672">=</span>tayeh-cluster-kubeconfig.yaml
kubectl patch ingress/harbor-ingress -p <span style="color:#e6db74">&#39;{&#34;spec&#34;: {&#34;ingressClassName&#34;: &#34;nginx&#34;}}&#39;</span> -n harbor --kubeconfig<span style="color:#f92672">=</span>tayeh-cluster-kubeconfig.yaml
</code></pre></div><p>check containers using</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>tayeh@fedora ~<span style="color:#f92672">]</span>$ kubectl --kubeconfig<span style="color:#f92672">=</span>tayeh-cluster-kubeconfig.yaml get pods -n harbor
NAME                                    READY   STATUS    RESTARTS   AGE
harbor-chartmuseum-7ddd5db67-2c7tf      1/1     Running   <span style="color:#ae81ff">0</span>          4m22s
harbor-core-6487f6cb4d-6qzz9            1/1     Running   <span style="color:#ae81ff">0</span>          4m22s
harbor-database-0                       1/1     Running   <span style="color:#ae81ff">0</span>          4m22s
harbor-jobservice-5f7c6784f4-plrz6      1/1     Running   <span style="color:#ae81ff">0</span>          4m22s
harbor-notary-server-5466b7fc4-w5s8h    1/1     Running   <span style="color:#ae81ff">2</span>          4m22s
harbor-notary-signer-77797d55f6-kx6wh   1/1     Running   <span style="color:#ae81ff">2</span>          4m22s
harbor-portal-7dcf769575-6mjkd          1/1     Running   <span style="color:#ae81ff">0</span>          4m22s
harbor-redis-0                          1/1     Running   <span style="color:#ae81ff">0</span>          4m22s
harbor-registry-7b7f7f547f-xvvmg        2/2     Running   <span style="color:#ae81ff">0</span>          4m22s
harbor-trivy-0                          1/1     Running   <span style="color:#ae81ff">0</span>          4m22s
</code></pre></div><p>and now you can access harbor via hostname <a href="http://harbor.tayeh.me">http://harbor.tayeh.me</a></p>
<blockquote>
<p>teraaform and value files uploaded to <a href="https://github.com/Tayeh/do-kubernetes-challenge">github</a></p>
</blockquote>
<p>and for more about <a href="https://www.digitalocean.com/community/pages/kubernetes-challenge">DigitalOcean Kubernetes Challenge</a></p>
<h4 id="thanks-for-reading">Thanks for reading</h4>
]]></content>
        </item>
        
        <item>
            <title>How to Install PHP 8 on CentOS 7/8 and Fedora</title>
            <link>https://tayeh.me/posts/install_php8_centos/</link>
            <pubDate>Sun, 29 Nov 2020 14:58:01 +0200</pubDate>
            
            <guid>https://tayeh.me/posts/install_php8_centos/</guid>
            <description>Install the EPEL repository  CentOS 8  dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm  CentOS 7  yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm  Fedora 33  dnf install https://rpms.remirepo.net/fedora/remi-release-33.rpm Install the Remi repository  CentOS 8  dnf install https://rpms.remirepo.net/enterprise/remi-release-8.rpm  CentOS 7  yum install https://rpms.remirepo.net/enterprise/remi-release-7.rpm  Fedora 33  dnf config-manager --set-enabled remi Install the yum-utils package  CentOS 8  dnf install yum-utils  CentOS 7  yum install yum-utils Enable the module stream for PHP 8.</description>
            <content type="html"><![CDATA[<h4 id="install-the-epel-repository">Install the EPEL repository</h4>
<ul>
<li>CentOS 8</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm
</code></pre></div><ul>
<li>CentOS 7</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
</code></pre></div><ul>
<li>Fedora 33</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">dnf install https://rpms.remirepo.net/fedora/remi-release-33.rpm
</code></pre></div><h4 id="install-the-remi-repository">Install the Remi repository</h4>
<ul>
<li>CentOS 8</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">dnf install https://rpms.remirepo.net/enterprise/remi-release-8.rpm
</code></pre></div><ul>
<li>CentOS 7</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">yum install https://rpms.remirepo.net/enterprise/remi-release-7.rpm
</code></pre></div><ul>
<li>Fedora 33</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">dnf config-manager --set-enabled remi
</code></pre></div><h4 id="install-the-yum-utils-package">Install the yum-utils package</h4>
<ul>
<li>CentOS 8</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">dnf install yum-utils
</code></pre></div><ul>
<li>CentOS 7</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">yum install yum-utils
</code></pre></div><h4 id="enable-the-module-stream-for-php-80">Enable the module stream for PHP 8.0:</h4>
<ul>
<li>CentOS 8</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">dnf module reset php
dnf module install php:remi-8.0
dnf update
</code></pre></div><ul>
<li>CentOS 7</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">yum-config-manager --disable <span style="color:#e6db74">&#39;remi-php*&#39;</span>
yum-config-manager --enable   remi-php80
yum update
</code></pre></div><ul>
<li>Fedora 33</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">dnf module reset php
dnf module install php:remi-8.0
dnf update
</code></pre></div><h4 id="install-php-8">Install PHP 8</h4>
<ul>
<li>CentOS 8</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">dnf install php
</code></pre></div><ul>
<li>CentOS 7</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">yum install php
</code></pre></div><ul>
<li>Fedora 33</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">dnf install php
</code></pre></div><p>To verify the installed version, use the php command:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">php -v
</code></pre></div><h4 id="conclusion">Conclusion</h4>
<p>So thatâ€™s it on how to install PHP 8 on CentOS and RHEL.</p>
<p>if you like this post consider to <strong><a href="/support">support me</a></strong></p>
]]></content>
        </item>
        
        <item>
            <title>How to install the NVIDIA drivers on Fedora 33 with Hybrid Switchable Graphics</title>
            <link>https://tayeh.me/posts/install_the_nvidia_drivers_on_fedora/</link>
            <pubDate>Wed, 25 Nov 2020 09:27:21 +0200</pubDate>
            
            <guid>https://tayeh.me/posts/install_the_nvidia_drivers_on_fedora/</guid>
            <description>This is guide, how to install NVIDIA proprietary drivers on Fedora 33 with Hybrid Switchable Graphics [Intel + Nvidia GeForce]
 Backup important files before you start installation. And this is of course at your own risk, because graphic cards, components and monitors are different and some combinations might cause totally unexpected results.
 identify your Nvidia graphic lspci -vnn | grep VGA the output of the above command will be like:</description>
            <content type="html"><![CDATA[<p>This is guide, how to install NVIDIA proprietary drivers on Fedora 33 with Hybrid Switchable Graphics [Intel + Nvidia GeForce]</p>
<blockquote>
<p>Backup important files before you start installation. And this is of course at your own risk, because graphic cards, components and monitors are different and some combinations might cause totally unexpected results.</p>
</blockquote>
<h3 id="identify-your-nvidia-graphic">identify your Nvidia graphic</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">lspci -vnn | grep VGA
</code></pre></div><p>the output of the above command will be like:</p>
<pre><code>00:02.0 VGA compatible controller [0300]: Intel Corporation UHD Graphics [8086:9bc4] (rev 05) (prog-if 00 [VGA controller])
01:00.0 VGA compatible controller [0300]: NVIDIA Corporation TU116M [GeForce GTX 1660 Ti Mobile] [10de:2191] (rev a1) (prog-if 00 [VGA controller])
</code></pre><h3 id="enable-rpm-fusion">Enable RPM fusion</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo dnf install https://mirrors.rpmfusion.org/free/fedora/rpmfusion-free-release-<span style="color:#66d9ef">$(</span>rpm -E %fedora<span style="color:#66d9ef">)</span>.noarch.rpm https://mirrors.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release-<span style="color:#66d9ef">$(</span>rpm -E %fedora<span style="color:#66d9ef">)</span>.noarch.rpm
</code></pre></div><h3 id="update-your-system">update your system</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo dnf update
</code></pre></div><blockquote>
<p>It is recommended to reboot your system after update</p>
</blockquote>
<h3 id="install-nvidia-driver">install Nvidia driver</h3>
<p><strong>install akmod-nvidia</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo dnf install gcc kernel-headers kernel-devel akmod-nvidia xorg-x11-drv-nvidia xorg-x11-drv-nvidia-libs xorg-x11-drv-nvidia-libs.i686
</code></pre></div><p><strong>install cuda</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo dnf install xorg-x11-drv-nvidia-cuda
</code></pre></div><p><strong>Drivers are installed, run the below command</strong></p>
<p>run <code>sudo akmods --force</code> and <code>sudo dracut --force</code> and then <code>reboot</code>.</p>
<p><strong>and that&rsquo;s it, the switch happens automatically when needed.</strong></p>
<blockquote>
<p>note this guide will disable wayland, X.org only supported.</p>
</blockquote>
<p><strong>to check NVIDIA Processes run the command <code>nvidia-smi</code></strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nvidia-smi
</code></pre></div><pre><code>Wed Nov 25 09:51:36 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 455.45.01    Driver Version: 455.45.01    CUDA Version: 11.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 166...  Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   33C    P8     4W /  N/A |      5MiB /  5944MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1776      G   /usr/libexec/Xorg                   4MiB |
+-----------------------------------------------------------------------------+
</code></pre><h4 id="screenshot">screenshot</h4>
<p><img src="/images/nvidia/nvidia_x_server_setting.png" alt="NVIDIA X Server Setting">
<img src="/images/nvidia/nvidia-smi.png" alt="nvidia-smi">
<img src="/images/nvidia/fedora_about.png" alt="fedora_about"></p>
]]></content>
        </item>
        
        <item>
            <title>How to automate a deploy with GitHub actions via SSH</title>
            <link>https://tayeh.me/posts/github_actions_via_ssh/</link>
            <pubDate>Tue, 24 Nov 2020 10:07:01 +0200</pubDate>
            
            <guid>https://tayeh.me/posts/github_actions_via_ssh/</guid>
            <description>Introduction GitHub Actions is an API for cause and effect on GitHub: orchestrate any workflow, based on any event, while GitHub manages the execution, provides rich feedback, and secures every step along the way.
In this article, we will be exploring a hands-on approach to managing your CD processes using GitHub Actions via SSH.
The workflow:
 Connect to VPS via SSH Move to project directory git pull the new changes execute any necessary command  Prerequisites  A GitHub account.</description>
            <content type="html"><![CDATA[<h3 id="introduction">Introduction</h3>
<p>GitHub Actions is an API for cause and effect on GitHub: orchestrate any workflow, based on any event, while GitHub manages the execution, provides rich feedback, and secures every step along the way.</p>
<p>In this article, we will be exploring a hands-on approach to managing your CD processes using GitHub Actions via SSH.</p>
<p>The workflow:</p>
<ol>
<li>Connect to VPS via SSH</li>
<li>Move to project directory</li>
<li><code>git pull</code> the new changes</li>
<li>execute any necessary command</li>
</ol>
<h4 id="prerequisites">Prerequisites</h4>
<ul>
<li>A GitHub account. If you donâ€™t have one, you can sign up here</li>
<li>A server with SSH access</li>
<li>Basic knowledge of writing valid YAML</li>
<li>Basic knowledge of GitHub and Git</li>
</ul>
<h4 id="configuring-workflows">Configuring workflows</h4>
<p>we should create a yml file on <code>.github/workflows/</code>. For example <code>.github/workflows/ci.yml</code>
and add this code to the file:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yml" data-lang="yml"><span style="color:#f92672">name</span>: <span style="color:#ae81ff">CI</span>

<span style="color:#f92672">on</span>: [<span style="color:#ae81ff">push]</span>

<span style="color:#f92672">jobs</span>:
  <span style="color:#f92672">deploy</span>:
    <span style="color:#f92672">if</span>: <span style="color:#ae81ff">github.ref == &#39;refs/heads/master&#39;</span>
    <span style="color:#f92672">runs-on</span>: [<span style="color:#ae81ff">ubuntu-latest]</span>
    <span style="color:#f92672">steps</span>:
      - <span style="color:#f92672">uses</span>: <span style="color:#ae81ff">actions/checkout@v1</span>
      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">Push to server</span>
        <span style="color:#f92672">uses</span>: <span style="color:#ae81ff">appleboy/ssh-action@master</span>
        <span style="color:#f92672">with</span>:
          <span style="color:#f92672">host</span>: <span style="color:#ae81ff">${{ secrets.SERVER_IP }}</span>
          <span style="color:#f92672">username</span>: <span style="color:#ae81ff">${{ secrets.SERVER_USERNAME }}</span>
          <span style="color:#f92672">key</span>: <span style="color:#ae81ff">${{ secrets.KEY }}</span>
          <span style="color:#f92672">passphrase</span>: <span style="color:#ae81ff">${{ secrets.PASSPHRASE }} </span>
          <span style="color:#f92672">script</span>: <span style="color:#ae81ff">cd ${{ secrets.PROJECT_PATH }} &amp;&amp; git pull</span>
</code></pre></div><p>After add this file go to <code>Settings -&gt; Secrets</code> and add secrets <code>SERVER_IP</code>, <code>SERVER_USERNAME</code>, <code>KEY</code>, <code>PASSPHRASE</code>, <code>PROJECT_PATH</code>
<img src="/images/github_actions_ssh_secrets.png" alt="github_actions_ssh_secrets"></p>
<blockquote>
<p>note: you can use <code>password</code> insted of <code>keys</code> just you need to replace the <code>key and passphrase</code> line with <code>password</code> in the workflow file <code>password: ${{ secrets.PASSWORD }}</code> and add the <code>password</code> to secrets</p>
</blockquote>
<blockquote>
<p>I use the GitHub secrets to keep important information hidden</p>
</blockquote>
<blockquote>
<p>also you can add more commands to the script line as you need</p>
</blockquote>
<p><strong>the next time we push to the master branch, it will automatically be deployed to our server.</strong></p>
<p><img src="/images/github_actions_run_job.png" alt="github_actions_run_job"></p>
]]></content>
        </item>
        
        <item>
            <title>Use SSH keys for authentication</title>
            <link>https://tayeh.me/posts/use-ssh-keys/</link>
            <pubDate>Tue, 17 Nov 2020 04:01:59 -1000</pubDate>
            
            <guid>https://tayeh.me/posts/use-ssh-keys/</guid>
            <description>Set up your first SSH keys Use SSH keys for authentication without password when you are connecting to your server. simple and secure login process.
To Generate a new SSH Key [root@server ~]$ ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.</description>
            <content type="html"><![CDATA[<h2 id="set-up-your-first-ssh-keys">Set up your first SSH keys</h2>
<p>Use SSH keys for authentication without password when you are connecting to your server. simple and secure login process.</p>
<h3 id="to-generate-a-new-ssh-key">To Generate a new SSH Key</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@server ~<span style="color:#f92672">]</span>$ ssh-keygen 
Generating public/private rsa key pair.
Enter file in which to save the key <span style="color:#f92672">(</span>/root/.ssh/id_rsa<span style="color:#f92672">)</span>: 
Enter passphrase <span style="color:#f92672">(</span>empty <span style="color:#66d9ef">for</span> no passphrase<span style="color:#f92672">)</span>: 
Enter same passphrase again: 
Your identification has been saved in /root/.ssh/id_rsa.
Your public key has been saved in /root/.ssh/id_rsa.pub.
The key fingerprint is:
SHA256:kxPyLTxxqwobFXoOxxxABaDD0xxnZzCB6xxxf38 root@server
The key<span style="color:#960050;background-color:#1e0010">&#39;</span>s randomart image is:
+---<span style="color:#f92672">[</span>RSA 2048<span style="color:#f92672">]</span>----+
|<span style="color:#f92672">=</span>+<span style="color:#f92672">==</span>*            |
|xo.o <span style="color:#f92672">=</span>           |
|+oo.O .          |
|<span style="color:#f92672">=</span>o.* * o         |
|.x. <span style="color:#f92672">=</span> X S        |
| .   O *         |
|    o <span style="color:#f92672">=</span> o        |
|     + qo.x.   P |
|    . .xx+o..o.  |
+----<span style="color:#f92672">[</span>SHA256<span style="color:#f92672">]</span>-----+

</code></pre></div><h3 id="first-way-copy-the-public-key-to-your-server-using-the-command">First way: Copy the public key to your server using the command</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@server ~<span style="color:#f92672">]</span>$ ssh-copy-id root@&lt;instance_ip&gt;
/usr/bin/ssh-copy-id: INFO: Source of key<span style="color:#f92672">(</span>s<span style="color:#f92672">)</span> to be installed: <span style="color:#e6db74">&#34;/root/.ssh/id_rsa.pub&#34;</span>
The authenticity of host <span style="color:#e6db74">&#39;&lt;instance_ip&gt; (&lt;instance_ip&gt;)&#39;</span> can<span style="color:#e6db74">&#39;t be established.
</span><span style="color:#e6db74">ECDSA key fingerprint is SHA256:aF/iyxxxKqx1LUyM/uyr/xxxxxxxxxxx.
</span><span style="color:#e6db74">ECDSA key fingerprint is MD5:xx:c3:xx:48:b4:ef:xx:e4:58:a4:xx:14:c1:xx:c5:af.
</span><span style="color:#e6db74">Are you sure you want to continue connecting (yes/no)? yes
</span><span style="color:#e6db74">/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
</span><span style="color:#e6db74">/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
</span><span style="color:#e6db74">root@&lt;instance_ip&gt;&#39;</span>s password: 

Number of key<span style="color:#f92672">(</span>s<span style="color:#f92672">)</span> added: <span style="color:#ae81ff">1</span>

Now try logging into the machine, with:   <span style="color:#e6db74">&#34;ssh &#39;root@&lt;instance_ip&gt;&#39;&#34;</span>
and check to make sure that only the key<span style="color:#f92672">(</span>s<span style="color:#f92672">)</span> you wanted were added.

</code></pre></div><h3 id="second-way-download-the-public-key-to-your-server-using-the-github-gitlab">Second way: Download the public key to your server using the Github, Gitlab</h3>
<ul>
<li>upload your key to Github or Gitlab: <code>settings -&gt;  SSH keys -&gt; New SSH key</code></li>
<li>after uoload the SSH key you can access key on <a href="https://github.com/tayeh.keys">Github</a>, <a href="https://gitlab.com/tayeh.keys">Gitlab</a></li>
</ul>
<p>now you can import the SSH key using curl command</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@server ~<span style="color:#f92672">]</span>$ curl -L https://github.com/tayeh.keys &gt;&gt; ~/.ssh/authorized_keys
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@server ~<span style="color:#f92672">]</span>$ curl -L https://github.com/tayeh.keys &gt;&gt; ~/.ssh/authorized_keys
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
<span style="color:#ae81ff">100</span>  <span style="color:#ae81ff">1315</span>  <span style="color:#ae81ff">100</span>  <span style="color:#ae81ff">1315</span>    <span style="color:#ae81ff">0</span>     <span style="color:#ae81ff">0</span>   <span style="color:#ae81ff">6015</span>      <span style="color:#ae81ff">0</span> --:--:-- --:--:-- --:--:--  <span style="color:#ae81ff">6032</span>
</code></pre></div><blockquote>
<p>note this way will import all keys on your github account</p>
</blockquote>
<p>now you can access your server without password try:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh root@&lt;instance_ip&gt;
</code></pre></div><h3 id="turn-off-password-authentication">Turn off password authentication</h3>
<p>With SSH key authentication, you can disable password authentication for SSH to prevent brute-forcing.
<strong>open SSH configuration file</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">vim /etc/ssh/sshd_config
</code></pre></div><p><strong>search for</strong> <code>PasswordAuthentication</code> and <code>PermitRootLogin</code> change it to:</p>
<pre><code>PasswordAuthentication no
PermitRootLogin without-password
</code></pre><p><strong>Restart the SSH service</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">systemctl restart sshd
</code></pre></div><h3 id="conclusions">Conclusions</h3>
<p>Remember to always keep your private keys safe.</p>
]]></content>
        </item>
        
    </channel>
</rss>
